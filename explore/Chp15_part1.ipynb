{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15장 Chapter 1 ~ 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==1.0.0-alpha0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ap3xwQiQJeIr",
        "outputId": "4ba63b76-2e95-4b50-836c-c15a9ac2624c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: d2l==1.0.0-alpha0 in /usr/local/lib/python3.7/dist-packages (1.0.0a0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha0) (0.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha0) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha0) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha0) (2.23.0)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha0) (1.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha0) (1.3.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha0) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha0) (1.7.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha0) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->d2l==1.0.0-alpha0) (0.16.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha0) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha0) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha0) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha0) (4.10.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha0) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha0) (7.7.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha0) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha0) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha0) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha0) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (57.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter->d2l==1.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha0) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha0) (1.1.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha0) (3.6.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha0) (5.4.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha0) (4.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha0) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha0) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha0) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha0) (23.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha0) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==1.0.0-alpha0) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l==1.0.0-alpha0) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha0) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha0) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->d2l==1.0.0-alpha0) (4.1.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha0) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha0) (5.0.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha0) (0.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha0) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha0) (1.5.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha0) (2.16.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha0) (4.3.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha0) (5.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha0) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha0) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha0) (4.12.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha0) (3.8.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0-alpha0) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==1.0.0-alpha0) (2022.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l==1.0.0-alpha0) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==1.0.0-alpha0) (21.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha0) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Natural Language Processing : Pretraining"
      ],
      "metadata": {
        "id": "0sg4btZZNAMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "ADYm5ONeNEsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "사람들은 의사소통할 필요가 있습니다.\n",
        "\n",
        "사람들에 대한 기본적인 조건이기도 하지만, 많은 양의 text가 매일 생성되어 왔습니다.\n",
        "\n",
        "미디어, 채팅 앱, 이메일, 리뷰, 뉴스, 논문, 책에 있는 풍부한 text를 주면, 컴퓨터가 사람들의 언어에 기반으로 text를 이해해서 도움을 주거나 결정을 내릴 수 있습니다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-yg4Dk_FNHYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP는 자연어를 사용하는 컴퓨터와 사람 사이의 상호작용을 연구합니다.\n",
        "\n",
        "실질적으로, text 데이터를 처리하거나 분석하기 위해 NLP 기술을 사용하는게 일반적입니다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Q80zvOvOOP0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "text를 이해하려면 representation부터 공부해야 합니다.\n",
        "\n",
        "거대한 말뭉치에있는 text sequence를 활용하기 위해서, 주변의 단어들로 숨겨진 단어가 무엇인지를 예측하는 것같이,Self-Supervised Learning은 text representation을 사전학습하는데 널리 사용되고 있습니다.\n",
        "\n",
        "이 방법으로, 모델은 비싼 labeling의 노력 없이 엄청난 text data로 지도학습합니다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Bi6qNm8POmM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = 'https://d2l.ai/_images/nlp-map-pretrain.svg' width = '500px' >\n"
      ],
      "metadata": {
        "id": "vnnfEHpITftY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번 장에서는, 각 단어나 subword를 개별 token으로 다룰 때, 개별 token의 representation은 거대한 말뭉치에서 word2vec, GloVe, Subword Embedding Model을 사용하여 pretraining에 사용됩니다.\n",
        "\n",
        "Pretraining 이후에, 개별 token의 representaion은 vector가 되지만, 문맥을 반영하진 못합니다.<br>(Example : 먹는 배, 타는 배 구별 X)\n",
        "\n",
        "그래서, 최신의 많은 pretraining 모델들은 다른 문맥에서 동일한 token의 representation을 적용합니다.\n",
        "\n",
        "그 중하나가 BERT이고, Transformer Encoder를 기반으로 한 더 깊은 Self-Supervised Model입니다.\n",
        "\n",
        "위에 그림처럼 어떻게 representation을 공부할지 볼 예정입니다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kBi-yUG-QQhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15-1. Word Embedding (Word2Vec)"
      ],
      "metadata": {
        "id": "c4gnEkH7XpQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "VB0EO7v_Xwgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "자연어는 의미를 표현하는 복잡한 시스템입니다.\n",
        "\n",
        "이 시스템에서, 단어는 의미를 가지는 기본 단위입니다.\n",
        "\n",
        "챕터 이름이 암시하듯이, Word Vector는 단어를 나타내는 벡터이고, 단어의 representation이자 feature vector로 여겨질 수 있습니다.\n",
        "\n",
        "실제 vector에 단어를 연결하는 기술은 word embedding이라고 불립니다.\n",
        "\n",
        "최근 몇년간, 단어 임베딩은 NLP의 기본 지식 되어오고 있습니다."
      ],
      "metadata": {
        "id": "R3wIujdGXxzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) One Hot Vectors Are a Bad Choice"
      ],
      "metadata": {
        "id": "7zitYk4giDgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.5장에서 단어를 나타내기 위해 one-hot 벡터를 사용했습니다.\n",
        "\n",
        "단어 사전에 서로 다른 단어의 수가 $N$개라고 가정하면, 각 단어는 $0$부터 $N-1$까지 서로 다른 숫자에 대응(매칭)됩니다.\n",
        "\n",
        "index $i$를 가지는 단어의 one-hot 벡터 representation을 얻으려면, index i만 1이고 나머지는 0인 길이 $N$의 벡터를 만들어야 합니다.\n",
        "\n",
        "이 방법으로, 각 단어는 길이가 $N$인 단어로 표현되고, NN에 바로 사용됩니다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-gHRtbpEiQT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-Hot 단어 벡터가 만들기는 쉽지만, 보통 좋은 선택은 아닙니다.\n",
        "\n",
        "주된 이유는 우리가 주요 사용하는 Cosine Similarity와 같이 단어 사이의 유사도를 정확히 나타낼 수 없기 때문입니다.\n",
        "\n",
        "$x,y\\in R^d$인 벡터들에게, Cosine 유사도는 그 각도의 Cosine 입니다.\n",
        "\n",
        "서로 다른 두 단어의 Cosine 유사도는 0이기 때문에, One-Hot Vector는 단어 사이의 유사도를 계산할 수 없습니다.\n",
        "\n",
        "<br><br>\n",
        "**단어 $\\mathbf{x}$와 단어 $\\mathbf{y}$의 Cosine Similarity**\n",
        "\n",
        "$$\\frac{\\mathbf{x}^\\top \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} \\in [-1, 1]$$\n",
        "\n",
        "\n",
        "- 이 값은 -1에서 1 사이에 존재\n",
        "- 한 단어가 1이고 다른 단어가 0이면, 두 단어 사이의 각도는 90도가 됨.\n",
        "- 한 단어가 0이고 다른 단어가 0이거나, 한 단어가 1이고 다른 단어가 1이면, 두 단어 사이의 각도는 0도가 됨.\n",
        "- Cosine0 = 1 (완전히 일치함)\n",
        "- Cosine1 = 0 (완전히 다름)\n",
        "\n",
        "=> 즉 단어 사이의 유사도를 볼 수는 없다.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6h-FOr_UiqCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Self-Supervised Word2Vec"
      ],
      "metadata": {
        "id": "pXF0AT0LlgEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec은 위에 문제를 해결하기 위해서 나왔다.\n",
        "\n",
        "W2V는 각 단어를 고정된 길이의 벡터로 매핑하고,<br> 각 벡터는 다른 단어들 사이의 관계를 분석하고 유사도를 나타낼 수 있다.\n",
        "\n",
        "W2V은 Skip-Gram과 CBOW의 2가지를 포함하고 있다.\n",
        "\n",
        "의미있는 representation에 대해서, 모델 학습은 말뭉치에서 주변 단어들로 어떤 단어들을 예측한 것처럼 조건부 확률에 의존한다.\n",
        "\n",
        "label이 없는 데이터를 학습하기 때문에, skip-gram과 Continuous Bag of Words는 둘 다 Self-Supervised 모델이다.\n",
        "\n",
        "앞으로 이 2개 모델과 학습 방법을 소개할 예정이다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "S2bjjsNlli5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) The Skip-Gram Model"
      ],
      "metadata": {
        "id": "J1nhvrGRrWrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip-Gram 모델은 text sequence에서 한 단어로 주변 단어를 만드는데 사용할 수 있다고 가정한다.\n",
        "\n",
        "\"the\", \"man\", \"loves\", \"his\", \"son\" 이란 문장을 생각해보자.\n",
        "\n",
        "\"loves\"를 Center Word로 선택하고, Window Size를 2로 설정하자.\n",
        "\n",
        "$$ P(\\textrm{\"the\"},\\textrm{\"man\"},\\textrm{\"his\"},\\textrm{\"son\"}\\mid\\textrm{\"loves\"}). $$\n",
        "\n",
        "위의 수식과 같이, 중심단어 \"loves\"가 주어졌을 때, Skip-Gram 모델은 2칸 이내의 문맥 단어 \"the\", \"man\", \"his\", \"son\"을 만들어내는 조건부 확률을 고려한다.\n",
        "\n",
        "문맥단어는 중심 단어가 주어졌을 때 독립적으로 만들어진다고 가정하자. (i.e : 조건부 독립)\n",
        "\n",
        "이 경우, 위의 조건부 확률은 아래와 같이 적을 수 있다.\n",
        "\n",
        "$$ P(\\textrm{\"the\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"man\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"his\"}\\mid\\textrm{\"loves\"})\\cdot P(\\textrm{\"son\"}\\mid\\textrm{\"loves\"}). $$\n",
        "<br><br>\n",
        "\n",
        "\n",
        "**[정리]**\n",
        "\n",
        "- 중심 단어 c와 문맥 단어 o, window size를 정하자.\n",
        "\n",
        "- 그럼 중심 단어 c로부터 window size 이내에 단어들이 문맥 단어 o가 된다.\n",
        "\n",
        "- Skip-Gram은 중심 단어 c로 문맥 단어 o를 만들어 낼 수 있다고 가정한다. <br>(어떤 문장에서 하나의 뭉텅이로 같이 다니는 단어들이 있을 것이고, 그 값을 계산하기 위한 모델이라고 생각하면 편하다. ex : 아침 + ?)\n",
        "\n",
        "- 중심 단어 c가 만들어내는 문맥 단어 o는 서로 독립이기 때문에, 개별 조건부 확률로 변환해서 나타낼 수 있다.<br>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "eaJQO9TJraHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip-Gram 모델에서, 각 단어는 조건부 확률을 계산하는데 2개의 d 차원 벡터 representations을 가진다.\n",
        "\n",
        "더 정확하게, 단어 사전에서 index $i$를 가지는 어떤 단어는, 중심 단어로 사용할 때는 $\\mathbf{v}_i \\in \\mathbb{R}^b$로, 문맥 단어로 사용할 때는 $\\mathbf{u}_i \\in \\mathbb{R}^b$로 표기한다.\n",
        "\n",
        "중심 단어 $w_c$가 주어졌을 때, 문맥 단어 $w_o$를 만들어내는 조건부 확률은 벡터 점곱의 softmax로 나타낼 수 있다.\n",
        "\n",
        "$$P(w_o \\mid w_c) = \\frac{\\text{exp}(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)},$$\n",
        "\n",
        "\n",
        "단어의 index set가 $V = \\{0, 1, ..., |V|-1\\}$를 만족한다.\n",
        "\n",
        "길이가 $T$인 text sequence가 주어졌을 때, $t$번째 단어를 $w^{(t)}$로 표기한다.\n",
        "\n",
        "문맥 단어는 중심 단어가 주어졌을 때 독립적으로 생성된다고 가정한다.\n",
        "\n",
        "Contxt Window Size $m$가 주어졌을 때, Skip-Gram 모델의 우도 함수는 어떤 중심 단어가 주어졌을 때 모든 문맥 단어를 만들어낼 확률이다.\n",
        "\n",
        "$$\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)})$$\n",
        "\n",
        "여기서, time step은 1보다 작거나 $T$보다 클 수 없다.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "**[정리]**\n",
        "- 중심 단어를 나타내는 Matrix 한 개와 문맥 단어를 나타내는 Matrix 한 개가 각각 존재\n",
        "\n",
        "- 중심 단어는 중심 단어 Matrix에서, 자신의 단어에 값을 뽑아와 $w_c$로 표기하고, <br>문맥 단어는 문맥 단어 Matrix에서 뽑아와 $w_o$로 표기한다.\n",
        "\n",
        "- 그럼 중심단어 c가 나타났을 때, 문맥 단어 o가 나타날 확률 $P($w_o$ | $w_c$)$을 계산할 수 있다.<br> 그 값은 Softmax를 통과시키는데, <br>$∑_{i \\in V}\\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)$는 중심 단어 c가 나타났을 때의 모든 문맥 단어들의 vector들에 대한 점곱의 exp의 합을, <br> $\\text{exp}(\\mathbf{o}_i^\\top \\mathbf{v}_c)$ 특정 문맥 단어 o에 대한 값이므로, <br>\n",
        "중심 단어 c가 나타났을 때 문맥 단어 o가 나타날 확률이 된다.\n",
        "\n",
        "- 이걸 Skip-Gram에서의 우도함수로 나타내보자.\n",
        "  - t번째 단어를 $w^{(t)}$라고 하자.\n",
        "  - 그럼 위에서 정의한 $P(w_o|w_c)$를 사용를 이용해서 단어들을 만들어낼 수 있다.\n",
        "  - $\\prod_{t=1}^{T}$ : 단어 위치는 1부터 마지막 단어까지 돌아가면서 곱\n",
        "  - $\\prod_{-m \\leq j \\leq m,\\ j \\neq 0}$ : 자기 자신을 의미하는 $j = 0$을 제외하고, 문맥 단어들이 만들어질 확률을 곱\n",
        "  - $ P(w^{(t+j)} \\mid w^{(t)})$ : 중심 단어가 주어졌을 때, 문맥 단어가 만들어질 확률\n",
        "  - $\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)})$ : t를 1부터 T까지 옮겨가면서, 중심 단어로 문맥 단어를 만들어낼 확률을 계산하여 곱<br>즉, 이 문장이 나타날 확률을 계산할 수 있다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hUJTW6bWt0ou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**학습**\n",
        "\n",
        "Skip-Gram 모델 파라미터는 단어 사전에 있는 각 단어에 대한 중심 단어 벡터와 문맥 단어 벡터이다.\n",
        "\n",
        "학습할 때, 우도 함수를 최대로 하도록 모델 파라미터를 학습한다. (i.e, 최대 우도 추정법)\n",
        "\n",
        "우도 함수를 최대화 하는 것은, Loss 함수를 최소화 하는 것과 동일하다.\n",
        "\n",
        "$$- \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m,\\ j \\neq 0} \\text{log}\\, P(w^{(t+j)} \\mid w^{(t)}).$$\n",
        "\n",
        "\n",
        "SGD를 이용해서 loss를 최소화 할 때, 매 iteration에서 짧은 subsequence를 뽑아내어 모델 파라미터를 갱신하기 위해 subsequence에 대한 미분 값을 계산한다.\n",
        "\n",
        "미분값을 계산하려면, 중심 단어 벡터와 문맥 단어 벡터에 대해 개별적으로 로그 조건부 확률의 미분 값을 얻어야 한다.\n",
        "\n",
        "일반적으로, 중심 단어 $w_c$와 문맥 단어 $w_o$에 대한 로그 조건부 확률은 다음의 수식을 만족한다.\n",
        "\n",
        "$$\\log P(w_o \\mid w_c) =\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).$$\n",
        "\n",
        "\n",
        "미분을 통해서, 중심 단어 벡터 $\\mathbf{v}_c$에 관점에서 gradient는 다음과 같다.\n",
        "\n",
        "$$\\begin{split}\\begin{aligned}\\frac{\\partial \\text{log}\\, P(w_o \\mid w_c)}{\\partial \\mathbf{v}_c}&= \\mathbf{u}_o - \\frac{\\sum_{j \\in \\mathcal{V}} \\exp(\\mathbf{u}_j^\\top \\mathbf{v}_c)\\mathbf{u}_j}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\\\&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} \\left(\\frac{\\text{exp}(\\mathbf{u}_j^\\top \\mathbf{v}_c)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)}\\right) \\mathbf{u}_j\\\\&= \\mathbf{u}_o - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid w_c) \\mathbf{u}_j.\\end{aligned}\\end{split}$$\n",
        "\n",
        "\n",
        "위 수식은 $w_c$를 중심 단어로 하는 사전의 모든 단어에 대한 조건부 확률이 필요하다.\n",
        "\n",
        "학습 이후에, 단어 사전에서 $i$번째 단어에 대해서 $v_i$와 $u_i$를 얻을 수 있다.\n",
        "\n",
        "자연어 처리 응용에서, Skip-gram 모델에 중심 단어 벡터는 단어 representation으로 주로 사용된다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "stoW1m1pxjAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) The Continuous Bag of Words (CBOW) model\n"
      ],
      "metadata": {
        "id": "5voJgR7GZ9G8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW 모델은 Skip-Gram 모델과 비슷하다.\n",
        "\n",
        "Skip-Gram 모델과의 큰 차이는 text sequence에서 중심 단어가 주변의 문맥 단어로부터 생성된다는 가정이다.\n",
        "\n",
        "예를 들어, \"the man loves his son with loves\"라는 문장이 있다.<br> 중심 단어는 loves, 문맥 단어 window는 2라고 하자<br>CBOW 모델은 문맥 단어 \"the\", \"man\", \"his\", \"son\"을 가지고 \"loves\"를 생성하는 조건부 확률을 계산한다.\n",
        "\n",
        "<img src = 'https://d2l.ai/_images/cbow.svg' width = '500px'>\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pgUvS0koYQuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW 모델에서 여러 문맥 단어가 있기 때문에, 이 문맥 단어 벡터는 조건부 확률의 계산에서 평균으로 구해진다.\n",
        "\n",
        "특히, 단어 사전에서 $i$번째 단어에 대해서, 중심 단어는 $\\mathbf{v}_i$로, 문맥 단어는 $\\mathbf{u}_i$로 표기한다.\n",
        "\n",
        "어떤 중심 단어 $\\mathbf{w}_c$를 만드는 조건부 확률은 둘러싼 문맥 단어 $w_{o_1}, ... ,w_{o_{2m}}$으로, 다음의 수식을 만족한다.\n",
        "\n",
        "$$P(w_c \\mid w_{o_1}, \\ldots, w_{o_{2m}}) = \\frac{\\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_c^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}\\left(\\frac{1}{2m}\\mathbf{u}_i^\\top (\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}}) \\right)}.$$\n",
        "\n",
        "- 2m 인 이유는, window size가 m이라면 중심 단어 양 옆으로 2m개의 문맥 단어가 형성되기 때문\n",
        "\n",
        "간결하게 적자면,\n",
        "- $\\mathcal{W}_o= \\{w_{o_1}, \\ldots, w_{o_{2m}}\\}$\n",
        "- $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}} \\right)/(2m)$\n",
        "\n",
        "$$P(w_c \\mid \\mathcal{W}_o) = \\frac{\\exp\\left(\\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o\\right)}{\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)}.$$\n",
        "\n",
        "(즉, \"the\", \"man\", \"his\", \"son\"이 모두 등장 했을 때 중시 단어 c가 나올 exp / 모든 조합)\n",
        "\n",
        "문장의 길이가 $T$이고, $t$번째 단어를 $w^{(t)}$, window size가 m이라고 하자.\n",
        "\n",
        "CBOW 모델의 우도함수는 주어진 문맥 단어로 중심 단어를 만드는 확률임으로, 다음을 만족한다.\n",
        "\n",
        "$$\\prod_{t=1}^{T}  P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6ZgBtRnoaAe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**학습**\n",
        "\n",
        "CBOW 모델 학습은 Skip-Gram 모델 학습과 같다.\n",
        "\n",
        "CBOW 모델의 최대 우도 추정은 아래의 loss 함수를 최소화하는 것과 같다.\n",
        "\n",
        "$$-\\sum_{t=1}^T  \\text{log}\\, P(w^{(t)} \\mid  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}).$$\n",
        "\n",
        "(우도 Maximize -> - 부호를 붙여 Minimize)<br>($\\Pi$ -> log를 붙여 합으로)<br><br>\n",
        "\n",
        "\n",
        "$$ \\log\\,P(w_c \\mid \\mathcal{W}_o) = \\mathbf{u}_c^\\top \\bar{\\mathbf{v}}_o - \\log\\,\\left(\\sum_{i \\in \\mathcal{V}} \\exp\\left(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o\\right)\\right).$$\n",
        "\n",
        "\n",
        "위의 두 수식을 활용하여, 미분하면 어떤 문맥 단어 벡터 $v_{o_{i}}$에 대한 개별적인 미분 값을 얻을 수 있고, 수식은 다음과 같다.\n",
        "\n",
        "\n",
        "$$\\frac{\\partial \\log\\, P(w_c \\mid \\mathcal{W}_o)}{\\partial \\mathbf{v}_{o_i}} = \\frac{1}{2m} \\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} \\frac{\\exp(\\mathbf{u}_j^\\top \\bar{\\mathbf{v}}_o)\\mathbf{u}_j}{ \\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\bar{\\mathbf{v}}_o)} \\right) = \\frac{1}{2m}\\left(\\mathbf{u}_c - \\sum_{j \\in \\mathcal{V}} P(w_j \\mid \\mathcal{W}_o) \\mathbf{u}_j \\right).$$\n",
        "\n",
        "다른 단어 벡터의 미분 값은 같은 방식으로 얻을 수 있다.\n",
        "\n",
        "Skip-gram 모델과 다르게, CBOW 모델은 단어 representation으로 문맥 단어 벡터를 사용한다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5xBEqlnBckuQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Summary"
      ],
      "metadata": {
        "id": "O2jpay44eJ01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어 벡터는 단어를 represent하는데 사용하고, 단어에 대한 representation이나 feature vector로 여겨진다. <br>단어를 실제 벡터에 연결하는 기술을 word embedding이라고 부른다.\n",
        "\n",
        "- word2vec 기술은 Skip-Gram과 CBOW 모델 둘 다 포함한다.\n",
        "\n",
        "- Skip-Gram 모델은 text sequence에서 주변 단어를 만든다고 가정하는 반면, CBOW 모델은 문맥 단어로 중심 단어를 만든다고 가정한다."
      ],
      "metadata": {
        "id": "FyCESquVeWHK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15.2 Approximate Training"
      ],
      "metadata": {
        "id": "s2SDStsIpaP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "Vke3df2Ipdvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\log P(w_o \\mid w_c) =\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\text{exp}(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).$$ \n",
        "15.1에서 다룬 내용을 상기해보자. \n",
        "\n",
        "Skip-Gram 모델의 중심 아이디어는 주어진 중심 단어 $w_c$를 가지고 문맥 단어 $w_o$를 만들어내는 조건부 확률을 계산하는 softmax 계산을 사용했고, log loss로 계산했다. (위의 수식)\n",
        "<br><br>\n",
        "Softmax 계산의 본질 때문에, 어떤 문맥 단어도 단어 사전 $V$에 있을 수 있기 때문에, 단어 사전의 전체 크기만큼 아이템을 더해줘야 한다.\n",
        "\n",
        "결과적으로, Skip-Gram 모델과 CBOW 모델 둘 다 미분 계산에 합이 포함되어 있다.\n",
        "\n",
        "불행히도, 단어 사전이 크면 gradient에 대한 계산 비용이 커진다.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "앞서 얘기한 계산 비용을 낮추기 위해서, 이번 장에서는 2가지 추정 기법인 Negative Sampling과 Hierarchical Softmax를 소개하겠다.\n",
        "\n",
        "Skip-Gram과 CBOW 모델이 비슷하기 때문에, Skip-Gram 모델을 예시로 2가지 추정 기법을 소개하겠다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Eb5vjpdepe52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Negative Sampling"
      ],
      "metadata": {
        "id": "AhBbazZ9rosY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Sampling은 원래의 목적 함수를 수정한다.\n",
        "\n",
        "중심 단어 $w_c$에 대한 문맥 단어가 주어졌을 때, context window에 있는 어떤 문맥 단어 $w_o$는 다음 확률 모델의 사건으로 본다.\n",
        "\n",
        "$$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)$$\n",
        "\n",
        "- $\\sigma$는 sigmoid 활성화 함수를 의미한다.<br>$\\sigma(x) = \\frac{1}{1+\\exp(-x)}.$\n",
        "- 즉, 어떤 중심 단어 c와 문맥 단어 o가 함께 나올 확률은 1에 가깝도록 모델링 되어야한다."
      ],
      "metadata": {
        "id": "wEHpHmjDrrcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "word embedding을 학습하기 위해서 text에 있는 모든 결합 확률을 최대화 하는 것부터 시작하자.\n",
        "\n",
        "주어진 문장의 길이가 $T$이고, $t$번째 단어를 $w^{(t)}$로, window 크기를 m으로 표기하면, 다음의 결합 확률을 최대화 하는 것을 고려한다.\n",
        "\n",
        "$$\\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)}).$$\n",
        "\n",
        "- 중심 단어와 문맥 단어가 동시에 나오는 확률이 1이 되도록 모든 단어 들에 대해서 결합 확률을 최대화한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "kMt27iLgrsWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "그러나, 위의 수식은 Positive 예시들만 포함하는 사건들을 다룬다. 결과적으로, 위의 수식은 모든 단어 벡터가 무한대일 때 1이 된다. 당연히, 그런 결과는 의미 없다.\n",
        "\n",
        "이 목적 함수가 더 의미있게 만들기 위해서, negative sampling은 사전 정의된 분포에서 뽑힌 negative samples들을 추가한다."
      ],
      "metadata": {
        "id": "vP_HVsWLrsUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "중심 단어 $w_c$의 context window로부터 얻어지는 문맥 단어 $w_o$에 대한 사건을 $S$라고 하자.\n",
        "\n",
        "$w_o$를 포함하는 이 사건에 대해서, 사전 정의된 분포 $P(w)$에서 context window에 없는 K개의 noise word를 뽑아낸다.\n",
        "\n",
        "$w_c$의 context window에서 얻을 수 없는 noise 단어 $w_k(k = 1,...,K)$는 $N_K$라고 하자.\n",
        "\n",
        "positive 예제들과 negative 예제들 $S, N_1, ..., N_K$ 둘을 포함하는 이 사건들은 완전히 독립적이라고 가정하자.\n",
        "\n",
        "Negative Sampling은 다음의 결합확률로 적는다.\n",
        "\n",
        "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(w^{(t+j)} \\mid w^{(t)})$$\n",
        "\n",
        "여기에 $S, N_1, \\ldots, N_K$에 대한 조건부 확률도 추정하면 다음과 같다.\n",
        "\n",
        "$$P(w^{(t+j)} \\mid w^{(t)}) =P(D=1\\mid w^{(t)}, w^{(t+j)})\\prod_{k=1,\\ w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k).$$\n",
        "\n",
        "- $P(D=1\\mid w^{(t)}, w^{(t+j)})$ : 중심 단어와 문맥 단어가 동시에 나오는 경우는 1이 되어야 함\n",
        "- $\\prod_{k=1,\\ w_k \\sim P(w)}^K$ : P(w)의 사전 정의된 분포에서 K개의 Negative 단어들을 골라냄\n",
        "- $P(D=0\\mid w^{(t)}, w_k)$ : 그럼 그 단어들은 함께 나올 확률이 0이 되어야 함"
      ],
      "metadata": {
        "id": "-dctBUG_ump-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "text sequence에서의 $i_t$는 위치한 t번째 단어를 $w^{(t)}$를,<br>$h_k$는 noise 단어 $w_k$를 의미한다.\n",
        "\n",
        "주건부 확률의 관점에서 log loss는 다음을 만족한다.\n",
        "\n",
        "$$\\begin{split}\\begin{aligned}\n",
        "-\\log P(w^{(t+j)} \\mid w^{(t)})\n",
        "=& -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)\\\\\n",
        "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)\\\\\n",
        "=&-  \\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right).\n",
        "\\end{aligned}\\end{split}$$\n",
        "\n",
        "\n",
        "\b학습 단계에서 gradient에 대한 계산 비용이 단어 사전의 크기와 관계 없고, K와 선형적인 관계가 있음을 알 수 있다.\n",
        "\n",
        "K를 작게 설정하면, negative sampling에 대한 계산 비용이 작아진다."
      ],
      "metadata": {
        "id": "JTQw7-nhrsSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Hierarchical Softmax"
      ],
      "metadata": {
        "id": "YSMNToJxrsQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = 'https://d2l.ai/_images/hi-softmax.svg' width = '500px'>\n",
        "\n",
        "학습 방법에 대한 대안으로, Hierarchical Softmax는 위의 이미지와 같은 binary tree를 사용하며, 나무에서의 개별 leaf node는 단어 사전의 단어를 표현한다.\n",
        "\n",
        "\n",
        "binary tree에서 단어 $w$를 나타내는 root node부터 leaf node까지의 경로에서의 노드의 수를 $L(w)$라고 하자.\n",
        "\n",
        "$n(w,j)$는 이 경로에서 $j^{th}$노드를, 그에 대한 문맥 단어 벡터를 $\\mathbf{u}_{n(w, j)}$라고 하자.\n",
        "\n",
        "예를 들어, 위의 이미지에서 $L(w_3)$을 보자.\n",
        "\n",
        "Hierarchical Softmax는 다음의 조건부 확률을 추정한다.\n",
        "\n",
        "$$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\text{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right),$$\n",
        "\n",
        "- $\\text{leftChild}(n)$은 노드 $n$의 왼쪽 child node이다\n",
        "- 만약 $x$가 true이면, $[\\![x]\\!]=1$이고,<br>$x$가 false이면 $[\\![x]\\!]=-1$이다.\n",
        "\n",
        "설명하자면, 중심 단어 $w_c$가 주어졌을 때, 단어 $w_3$을 만들어내는 조건부 확률을 계산해보자.\n",
        "\n",
        "이 과정은 $w_c$의 단어 벡터 $v_c$와 root node에서 $w_3$로 가는 경로((좌, 우, 좌)) 위에 벡터들의 곱이 필요하며, 다음의 수식을 만족한다.\n",
        "\n",
        "$$P(w_3 \\mid w_c) = \\sigma(\\mathbf{u}_{n(w_3, 1)}^\\top \\mathbf{v}_c) \\cdot \\sigma(-\\mathbf{u}_{n(w_3, 2)}^\\top \\mathbf{v}_c) \\cdot \\sigma(\\mathbf{u}_{n(w_3, 3)}^\\top \\mathbf{v}_c).$$\n",
        "\n",
        "$\\sigma(x) + \\sigma(-x) = 1$을 만족하기 때문에, 어떤 $w_c$를 기반으로 단어 사전에 있는 모든 단어를 만들어내는 조건부 확률의 합은 1이 된다.\n",
        "\n",
        "$$\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.$$\n",
        "\n",
        "운 좋게도, binary tree에서 $L(w_o) - 1$이 $\\mathcal{O}(\\text{log}_2|\\mathcal{V}|)$차수 이기 때문에, 단어 사전의 크기가 클 수록 상대적으로 계산 비용이 작아진다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5gQtyvDdrsL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Summary\n",
        "\n",
        "- Negative Sampling은 positive와 negative 예제들을 포함하는 독립적 사건들을 고려하도록 loss 함수를 만들었다. 학습하는데에 계산 비용은 매 step에서의 noise 단어의 수에 선형적으로 의존적이다.\n",
        "\n",
        "- Hierarchical Softmax는 binary tree에서 root node에서 leaf node까지의 경로를 사용하여 loss 함수를 만들었다. 학습하는데에 계산 비용은 매 step에서 사전의 크기의 log에 의존적이다.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ROOOk9oUrsJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15-3. The Dataset for Pretraining Word Embeddings"
      ],
      "metadata": {
        "id": "yvsgMaTvrsHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "FSo--lWhrsFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W2V 모델과 Approximate Traing 방법에 대한 기술적 detail을 알고 있으니, 적용해보자.\n",
        "\n",
        "특히, Skip-Gram 모델과 Negative Sampling을 예제로 들어보자.\n",
        "\n",
        "이번 장에서, Word Embedding 모델을 pretraining하는데 필요한 dataset부터 시작하자.\n",
        "\n",
        "데이터의 원래 형태는 학습하는동안 미니배치 단위로 변환된다."
      ],
      "metadata": {
        "id": "gGvoTegOE0cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from d2l import torch as d2l\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "BOnNZPaRE0ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Reading the Dataset"
      ],
      "metadata": {
        "id": "7_eEqqICFPaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset은 Penn Tree Bank(PTB)를 사용한다.\n",
        "\n",
        "이 말뭉치는 Wall Street Journal 기사들을 뽑은 것이고, training, validation, test 셋으로 나눈다.\n",
        "\n",
        "원래 형태에서는, text file의 각 문장은 띄어쓰기 단위로 구분된 단어의 문장이다.\n",
        "\n",
        "여기서는 토큰을 단어로 보자."
      ],
      "metadata": {
        "id": "cVZ8WeSSFSRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',\n",
        "                       '319d85e578af0cdc590547f26231e4e31cdf1e42')\n",
        "\n",
        "def read_ptb():\n",
        "    \"\"\"Load the PTB dataset into a list of text lines.\"\"\"\n",
        "    data_dir = d2l.download_extract('ptb')\n",
        "    # Read the training set\n",
        "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
        "        raw_text = f.read()\n",
        "    return [line.split() for line in raw_text.split('\\n')]\n",
        "\n",
        "sentences = read_ptb()\n",
        "print(f'# sentences: {len(sentences)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptoAA-ZlFOqr",
        "outputId": "8f73a44e-378e-48b9-b131-227a4784c708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# sentences: 42069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training 데이터를 불러온 후, 말뭉치에서 10번 이상 나온 단어 사전을 만들고, 나머지는 \"\\<unk\\>\"토큰으로 대체한다.\n",
        "\n",
        "원본 데이터셋에서도 희귀한 단어는 \"\\<unk\\>\"로 나타낸다."
      ],
      "metadata": {
        "id": "lXn5Dc-rKxLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
        "        \"\"\"Defined in :numref:`sec_text-sequence`\"\"\"\n",
        "        # Flatten a 2D list if needed\n",
        "        if tokens and isinstance(tokens[0], list):\n",
        "            tokens = [token for line in tokens for token in line]\n",
        "        # Count token frequencies\n",
        "        counter = collections.Counter(tokens)\n",
        "        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                  reverse=True)\n",
        "        # The list of unique tokens\n",
        "        self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [\n",
        "            token for token, freq in self.token_freqs if freq >= min_freq])))\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if hasattr(indices, '__len__') and len(indices) > 1:\n",
        "            return [self.idx_to_token[int(index)] for index in indices]\n",
        "        return self.idx_to_token[indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return self.token_to_idx['<unk>']"
      ],
      "metadata": {
        "id": "pQ03zvpbIYXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = d2l.Vocab(sentences, min_freq=10)\n",
        "print(f'vocab size: {len(vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lnt8py2pKoxd",
        "outputId": "7ac7ebae-d3ac-493a-cfe7-14fdf7e4e74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 6719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Subsampling\n",
        "\n",
        "Text 데이터는 \"the\", \"a\"와 \"in\"과 같은 고빈도 단어가 있다.\n",
        "\n",
        "- 엄청 큰 말뭉치에서는 수십억번도 넘게 나올 것이다.\n",
        "\n",
        "그러나, 이 단어들은 유용한 신호는 주지도 않으면서, context window에서 많은 다른 단어들이랑 같이 나온다.\n",
        "\n",
        "- 저빈도 단어 \"intel\"이랑 같이 나온  경우가 고빈도 단어 \"a\"랑 같이 나온 경우보다 유용할 것이다.\n",
        "\n",
        "게다가, 학습 속도도 느려진다.\n",
        "\n",
        "그래서, embedding model을 학습할 때, 고빈도 단어는 제외된다.\n",
        "\n",
        "특히, 데이터셋에서 단어 $w_i$는 다음의 확률적으로 버려진다.\n",
        "\n",
        "$$P(w_i) = \\max\\left(1 - \\sqrt{\\frac{t}{f(w_i)}}, 0\\right)$$\n",
        "\n",
        "- $f(w_i)$는 데이터셋에서 총 단어의 수에 대한 단어 $w_i$의 비율\n",
        "- 상수 $t$는 하이퍼 파라미터\n",
        "\n",
        "상대 빈도 $f(w_i) > t$이면 버려지고, 단어의 상대 빈도가 높ㅇ르수록, 버려질 가능성이 높아진다."
      ],
      "metadata": {
        "id": "EOXn5mtNLGVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample(sentences, vocab):\n",
        "    \"\"\"Subsample high-frequency words.\"\"\"\n",
        "    # 문장에서 unk는 제거\n",
        "    sentences = [[token for token in line if vocab[token] != vocab.unk]\n",
        "                 for line in sentences]\n",
        "\n",
        "    # 각 문장에서의 단어의 수를 카운팅 \n",
        "    counter = collections.Counter([\n",
        "        token for line in sentences for token in line])\n",
        "    \n",
        "    # \b총 단어 수\n",
        "    num_tokens = sum(counter.values())\n",
        "\n",
        "    def keep(token):\n",
        "        \"\"\"\n",
        "        1e-4 : t\n",
        "        f(w_i) : w_i 의 빈도 수 / 총 단어의 수\n",
        "        1/f(w_i) : 총 단어의 수 / w_i의 빈도수\n",
        "        => t/f(w_i) : t / w_i의 빈도수 * 총 단어의 수\n",
        "        => t / counter[token] * num_tokens\n",
        "        => 이 값이 1보다 크면 고빈도 단어로 판단해 버림\n",
        "        \"\"\"\n",
        "        return(random.uniform(0, 1) <\n",
        "               math.sqrt(1e-4 / counter[token] * num_tokens))\n",
        "\n",
        "    return ([[token for token in line if keep(token)] for line in sentences],\n",
        "            counter)\n",
        "\n",
        "subsampled, counter = subsample(sentences, vocab)"
      ],
      "metadata": {
        "id": "B1KBKGDpKrUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래의 코드는 subsampling 전 후의 문장당 토큰의 수를 나타낸다.\n",
        "\n",
        "예상했다시피, subsampling은 고빈도 단어를 버려 문장을 짧게 해주고, 학습 속도를 향상시킨다."
      ],
      "metadata": {
        "id": "5ZdrvUenTn0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist([\n",
        "          [len(x) for x in sentences], \n",
        "          [len(x) for x in subsampled]\n",
        "          ])\n",
        "plt.xticks([0,20,40,60,80])\n",
        "plt.xlabel(\"# tokens per sentence\")\n",
        "plt.ylabel('count')\n",
        "plt.legend(['origin', 'subsampled'])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "aM3kGaEOTlPG",
        "outputId": "d343c9d4-89e0-4c6b-cdd1-6eab29c5959c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "개별 토큰에 대해서, \"the\"가 추출될 확률도 1/20이 됐다.\n",
        "\n",
        "반면 join은 유지됐다."
      ],
      "metadata": {
        "id": "oTwljHq4Wj_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_counts(token):\n",
        "    return (f'# of \"{token}\": '\n",
        "            f'before={sum([l.count(token) for l in sentences])}, '\n",
        "            f'after={sum([l.count(token) for l in subsampled])}')\n",
        "\n",
        "print(compare_counts('the'))\n",
        "print(compare_counts('join'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-qjVWshWIuF",
        "outputId": "8d874453-cb54-4206-fd22-771234594413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of \"the\": before=50770, after=2056\n",
            "# of \"join\": before=45, after=45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음의 코드로 말뭉치의 index로 바꿔주었다."
      ],
      "metadata": {
        "id": "QzuCU8QtW0NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [vocab[line] for line in subsampled]\n",
        "\n",
        "print(subsampled[1])\n",
        "print(corpus[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xq_C7GljWnht",
        "outputId": "2f728934-07b7-485c-9150-76facfc97dcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['join', 'the', 'board']\n",
            "[3228, 6079, 710]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Extracting Center Words and Context Words"
      ],
      "metadata": {
        "id": "SRx57MiNXEg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "다음 함수로 모든 중심 단어와 그에 따른 문맥 단어를 추출한다.\n",
        "\n",
        "1부터 max_window_size까지의 정수를 context window size로 랜덤으로 균일하게 샘플링한다.\n",
        "\n",
        "어떤 center word에서 샘플링된 context window size를 초과하지 않으면 context 단어 이다."
      ],
      "metadata": {
        "id": "JL8Iy5bCXHxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_centers_and_contexts(corpus, max_window_size):\n",
        "    \"\"\"Return center words and context words in skip-gram.\"\"\"\n",
        "    centers, contexts = [], []\n",
        "\n",
        "    # 개별 말뭉치에 대해서\n",
        "    for line in corpus:\n",
        "        # 2 단어가 안되면 버림\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "\n",
        "        # 중심 단어에다가 모든 단어들 추가해두고\n",
        "        centers += line\n",
        "        # 중심 단어에 따른 문맥 단어들을 추가해준다.\n",
        "        for i in range(len(line)):  # Context window centered at `i`\n",
        "            # window size는 1부터 max까지 랜덤하게 설정하고\n",
        "            window_size = random.randint(1, max_window_size)\n",
        "            # 그 값을 알아서 sampling 한다.\n",
        "            indices = list(range(max(0, i - window_size),\n",
        "                                 min(len(line), i + 1 + window_size)))\n",
        "            # 중심 단어가 문맥 단어에 동시에 포함되면 안되니 그 단어는 버리고\n",
        "            indices.remove(i)\n",
        "            # 최종 문맥 단어를 추가한다.\n",
        "            contexts.append([line[idx] for idx in indices])\n",
        "    return centers, contexts"
      ],
      "metadata": {
        "id": "8w3KMP0FW5xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "잘 되는지 보자.\n",
        "\n",
        "7글자, 3글자가 있는 문장이라고 생각하자.\n",
        "\n",
        "max window size가 2일 경우, 결과는 다음과 같이 나올 수 있다."
      ],
      "metadata": {
        "id": "cNIy7SgPYD13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
        "print('dataset', tiny_dataset)\n",
        "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
        "    print('center', center, 'has contexts', context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czwgEvgpW-YU",
        "outputId": "c66dd91c-3301-4e59-aa27-e2daa21ef42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
            "center 0 has contexts [1, 2]\n",
            "center 1 has contexts [0, 2, 3]\n",
            "center 2 has contexts [0, 1, 3, 4]\n",
            "center 3 has contexts [1, 2, 4, 5]\n",
            "center 4 has contexts [2, 3, 5, 6]\n",
            "center 5 has contexts [3, 4, 6]\n",
            "center 6 has contexts [5]\n",
            "center 7 has contexts [8, 9]\n",
            "center 8 has contexts [7, 9]\n",
            "center 9 has contexts [8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "window size는 5로 하자."
      ],
      "metadata": {
        "id": "MjhLd3L5YV12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_centers, all_contexts = get_centers_and_contexts(corpus, 5)\n",
        "f'# center-context pairs: {sum([len(contexts) for contexts in all_contexts])}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "i1OgOnvxYCAC",
        "outputId": "99f1ba0a-a1f0-47a4-ac6b-70f93d037169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# center-context pairs: 1502623'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Negative Sampling"
      ],
      "metadata": {
        "id": "lJ6CSRX6Yb36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "negative sampling으로 추정 학습을 하자.\n",
        "\n",
        "사전 정의된 분포에서 noise 단어를 추출하기 위해서,<br>sampling distribution이 sampling weight를 따르는 Random Generator Class를 정의하자."
      ],
      "metadata": {
        "id": "yxHVh2NOYf8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomGenerator:\n",
        "    \"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\n",
        "    def __init__(self, sampling_weights):\n",
        "        # 1부터 Sampling Weights의 길이까지\n",
        "        self.population = list(range(1, len(sampling_weights) + 1))\n",
        "        self.sampling_weights = sampling_weights\n",
        "        self.candidates = []\n",
        "        self.i = 0\n",
        "\n",
        "    def draw(self):\n",
        "        # i와 candidates의 길이가 같으면,아래를 실행한다.\n",
        "        if self.i == len(self.candidates):\n",
        "            # 1부터 sampling weight까지의 숫자들을\n",
        "            # sampling weight의 비율을 맞추도록\n",
        "            # 10,000개의 단어를 내보낸다.\n",
        "            self.candidates = random.choices(\n",
        "                self.population, self.sampling_weights, k=10000)\n",
        "            # 이후 i를 다시 0으로 세팅하며 마무리한다.\n",
        "            self.i = 0\n",
        "        # i를 업데이트한다.\n",
        "        self.i += 1\n",
        "        # self.candidates에서 랜덤하게 뽑은 단어들 중에서 i -1 을 내뱉는다.\n",
        "        return self.candidates[self.i - 1]"
      ],
      "metadata": {
        "id": "RiSuDoqQYZRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래의 코드는 [1,2,3]을 뽑을 수 있으며,\n",
        "\n",
        "각각의 뽑힐 확률은 [2/9, 3/9, 4/9] 이다.\n",
        "\n",
        "이 때 10번 반복해서 추출하면 다음과 같다."
      ],
      "metadata": {
        "id": "mPHoL7poZ-Je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = RandomGenerator([2, 3, 4])\n",
        "[generator.draw() for _ in range(10)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHS1JdWrY6Ys",
        "outputId": "7fcdac08-5168-400c-aa88-d6771bfa7c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 2, 3, 3, 2, 3, 2, 3, 2, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "중심 단어와 문맥 단어에 대해서, 랜덤하고 K개의 noise 단어를 뽑아야 한다.\n",
        "\n",
        "W2V 논문의 제안에 따르면, noise word $w$를 추출하는 확률 $P(w)$는 그 단어의 상대 빈도에 0.75 제곱이다."
      ],
      "metadata": {
        "id": "g8U17LH3aLKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negatives(all_contexts, vocab, counter, K):\n",
        "    \"\"\"Return noise words in negative sampling.\"\"\"\n",
        "\n",
        "    # Sampling Weight는 단어의 숫자만큼 만들고, 각 단어 빈도의 0.75 제곱이다.\n",
        "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n",
        "                        for i in range(1, len(vocab))]\n",
        "\n",
        "    # all_negatives는 빈 리스트이고\n",
        "    # generator는 sampling weight를 반영한 Random Generator이다.\n",
        "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
        "\n",
        "    # 모든 문장들에 대해서,\n",
        "    for contexts in all_contexts:\n",
        "        negatives = []\n",
        "        \n",
        "        # negative의 길이가 문장 길이 * K보다 작으면\n",
        "        while len(negatives) < len(contexts) * K:\n",
        "            # 랜덤하게 추출해서\n",
        "            neg = generator.draw()\n",
        "            # 원래 문장에 없는 단어이면\n",
        "            if neg not in contexts:\n",
        "                # 추가하고 아니면 버림\n",
        "                negatives.append(neg)\n",
        "        # 완성된 negative 문장은 all_negatives에 추가하고\n",
        "        all_negatives.append(negatives)\n",
        "    # 모든 문장에 대해서 시행이 완료되면 반환한다.\n",
        "    return all_negatives\n",
        "\n",
        "# 모든 context vecotrs, vocab, counter와 K=5로 세팅\n",
        "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
      ],
      "metadata": {
        "id": "8N3qaRXYZHT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 길이가 3인 문장은 5배 하여 15개로 채움\n",
        "print(len(all_contexts[0]), len(all_negatives[0]))\n",
        "print(len(all_contexts[1]), len(all_negatives[1]))\n",
        "print(len(all_contexts[2]), len(all_negatives[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Fbv7XPbA7A",
        "outputId": "f885b3c5-9f51-4f63-dea3-fd56ae7e4dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 10\n",
            "2 10\n",
            "2 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Loading Trainig Examples in Minibatches"
      ],
      "metadata": {
        "id": "5HrkzydubuYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "모든 중심 단어와 문맥 단어, noise 단어들을 추출한 후에, 학습동안 반복적으로 추출하기 위해 minibatch로 만들어야 한다.\n",
        "\n",
        "minibatch에서, i번째 example은 중심 단어와 $n_i$ 문맥 단어, $m_i$ noise 단어를 포함한다.\n",
        "\n",
        "다양한 문맥 단어 크기 때문에, $n_i + m_i$는 i에 따라서 다른다.\n",
        "\n",
        "따라서, 개별 예제에 대해서 문맥 단어와 noise 단어를 contexts_negatives라는 변수에 합치고, $max_i n_i+m_i$길이 만큼 0으로 채운다.\n",
        "\n",
        "0은 loss 계산에서 제외하기 위해서, mask 변수를 선언한다.\n",
        "\n",
        "contexts_negatives라는 변수에 패딩에 맞는 0짜리 mask에 대해서, contexts_negatives에 있는 마스크와 원소는 1:1로 매칭된다.\n",
        "\n",
        "positive 와 negative 변수를 구별하기 위해서, contexts_negatives에 있는 noise 단어로부터 context words를 분리한다.\n",
        "\n",
        "mask와 비슷하게, label과 context_negatives에 맞는 원소들 사이에 label이면 1이고 패딩이면 0이도록 마스킹 한다.\n",
        "\n",
        "위의 아이디어는 batchify 함수로 구현됐다.\n",
        "\n",
        "batch size 길이에 맞게 리스트로 넣고, 중심 단어, 문맥 단어, noise 단어들을 구성하도록 원소를 만든다."
      ],
      "metadata": {
        "id": "Khcv8V2Xb8hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data):\n",
        "\n",
        "    # 문장의 최대 길이를 계산해야 한다 : window size가 랜덤이기 때문에, 제일 큰 값을 기준으로 잡는다.\n",
        "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
        "    # 중심 단어, 문맥 단어, 마스크 (패딩 지우기) , labels(실제 문맥 단어)\n",
        "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
        "\n",
        "    # 주어진 데이터는 [중심단어, 문맥단어, noise 단어]를 문장 단위로 주고\n",
        "    for center, context, negative in data:\n",
        "        # 현재 길이는 문맥 단어 길이 + nosie 단어 길이이다.\n",
        "        cur_len = len(context) + len(negative)\n",
        "        # 일단 중심 단어에다가는 중심 단어들 추가해두고\n",
        "        centers += [center]\n",
        "        \n",
        "        # context + negative를 한 후에, max_len보다 부족한 수는 0으로 패딩으로 채운다.\n",
        "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
        "\n",
        "        # 그럼 0은 사실상 의미없는 단어이기 때문에, masking을 해줘야 한다.\n",
        "        # 따라서 의미 있는 단어는 1로 채워 가져오고, 의미 없는 단어는 0으로 채워 가져온다.\n",
        "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
        "\n",
        "        # 그리고 loss함수 계산할 때, 문맥 단어랑 noise 단어는 따로 계산했었다.\n",
        "        # 그에 대한 마스킹 처리를 해준다.\n",
        "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
        "    \n",
        "    # 모든 문장에 대해서 완료되면, tensor로 변환해서 본다.\n",
        "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\n",
        "        contexts_negatives), torch.tensor(masks), torch.tensor(labels))"
      ],
      "metadata": {
        "id": "4Tr3R2c1bVoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "x_1 = (1, [2, 2], [3, 3, 3, 3])\n",
        "x_2 = (1, [2, 2, 2], [3, 3])\n",
        "batch = batchify((x_1, x_2))\n",
        "\n",
        "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
        "for name, data in zip(names, batch):\n",
        "    print(name, '=', data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgWO0suCehpM",
        "outputId": "71a82bfd-38f3-4c45-d123-b971ebcfe7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centers = tensor([[1],\n",
            "        [1]])\n",
            "contexts_negatives = tensor([[2, 2, 3, 3, 3, 3],\n",
            "        [2, 2, 2, 3, 3, 0]])\n",
            "masks = tensor([[1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 0]])\n",
            "labels = tensor([[1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0, 0]])\n",
            "CPU times: user 3.41 ms, sys: 41 µs, total: 3.45 ms\n",
            "Wall time: 3.81 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Putting All Things Together"
      ],
      "metadata": {
        "id": "Y6KS7bZIelqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
        "    \"\"\"데이터 불러오기\"\"\"\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    sentences = read_ptb()\n",
        "\n",
        "    vocab = Vocab(sentences, min_freq=10)\n",
        "    subsampled, counter = subsample(sentences, vocab)\n",
        "    corpus = [vocab[line] for line in subsampled]\n",
        "    all_centers, all_contexts = get_centers_and_contexts(\n",
        "        corpus, max_window_size)\n",
        "    all_negatives = get_negatives(\n",
        "        all_contexts, vocab, counter, num_noise_words)\n",
        "\n",
        "    class PTBDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, centers, contexts, negatives):\n",
        "            assert len(centers) == len(contexts) == len(negatives)\n",
        "            self.centers = centers\n",
        "            self.contexts = contexts\n",
        "            self.negatives = negatives\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            return (self.centers[index], self.contexts[index],\n",
        "                    self.negatives[index])\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.centers)\n",
        "\n",
        "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
        "\n",
        "    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,\n",
        "                                      collate_fn=batchify,\n",
        "                                      num_workers=num_workers)\n",
        "    return data_iter, vocab"
      ],
      "metadata": {
        "id": "Zv_eJrYzej5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "data_iter, vocab = load_data_ptb(512, 5, 5)\n",
        "for batch in data_iter:\n",
        "    for name, data in zip(names, batch):\n",
        "        print(name, 'shape:', data.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJXVSrhaezBL",
        "outputId": "5e63b9a2-0960-4611-a4ff-6bbbbf9e5854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "centers shape: torch.Size([512, 1])\n",
            "contexts_negatives shape: torch.Size([512, 60])\n",
            "masks shape: torch.Size([512, 60])\n",
            "labels shape: torch.Size([512, 60])\n",
            "CPU times: user 14.7 s, sys: 262 ms, total: 15 s\n",
            "Wall time: 15.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7) 요약\n",
        "- 고빈도 단어는 학습에 유용하지 않다.<br>학습 속도 향상을 위해 뽑아내야 한다.\n",
        "\n",
        "- 계산 효율을 위해, minibatch 단위로 데이터를 뽑는다.<br>padding과 non 패딩으로 변수를 구분하고, positive 예제와 negative 예제로 분리한다."
      ],
      "metadata": {
        "id": "iXKf8-MHe2TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15.4 Pretraining Word2Vec"
      ],
      "metadata": {
        "id": "Jo74de8Sfewx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "_KcWEvwJfhZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip-Gram 모델을 만들어보자.\n",
        "\n",
        "그러고 PTB 데이터로 negative sampling을 활용해서 word2vec을 사전학습 할 것이다."
      ],
      "metadata": {
        "id": "c5yU2h1Gfi4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) The Skip-Gram Model"
      ],
      "metadata": {
        "id": "B9U6v50ZfwRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Layer와 Batch Matrix 연산으로 Skip-Gram 모델을 만들자.\n",
        "\n",
        "우선, embedding layer가 작동하는 것부터 보자."
      ],
      "metadata": {
        "id": "9RDEzu3ifzJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(1) Embedding Layer**"
      ],
      "metadata": {
        "id": "uO5i8EpJf6KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding Layer는 token의 index로 feature vector에 매핑한다.\n",
        "\n",
        "layer의 weight는 matrix이고<br>row의 수는 단어 사전의 크기와 동일하고(input_dim)<br>columns의 수는 개별 토큰의 벡터와 동일하다."
      ],
      "metadata": {
        "id": "H5ofhLpYf_PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
        "print(f'Parameter embedding_weight ({embed.weight.shape}, '\n",
        "      f'dtype={embed.weight.dtype})')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfsw3yCGe1DB",
        "outputId": "d56c3668-705c-4874-e7cd-a22d5ce3739d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter embedding_weight (torch.Size([20, 4]), dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "embdding layer의 input은 단어 token의 index이다.\n",
        "\n",
        "어떤 토큰 index i에 대해서, vector representation은 embedding layer에서 weight matrixdml i번째 row에서 얻을 수 있다.\n",
        "\n",
        "vector dimension을 4로 설정했기 때문에, (2,3)의 minibatch token에 대해서 (2,3,4)의 벡터를 반환한다."
      ],
      "metadata": {
        "id": "tJVqB-f-goqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "embed(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRilBxrZgfgY",
        "outputId": "a1a28bb7-ebba-4ddb-cb23-0d28c5b5ad26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.2329, -0.4490,  1.8511, -0.3472],\n",
              "         [-0.8467, -0.5936, -1.2526, -0.6882],\n",
              "         [ 0.0939, -0.4390,  0.9516, -0.4169]],\n",
              "\n",
              "        [[-0.0582,  1.2687, -1.1308,  1.0305],\n",
              "         [ 0.1055, -0.9912,  0.9447, -1.1573],\n",
              "         [ 0.0940,  0.6034,  0.5385, -1.5726]]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(2) Defining the Forward Propagation**"
      ],
      "metadata": {
        "id": "0Gtz5f00i6AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "순전파에서, skip-gram 모델의 input은 center word (batch_size, 1)의 모양과<br> context와 noise word를 합친 contexts_and_negatives의 (batch_size, max_len)이다.\n",
        "\n",
        "두 변수는 우선 embedding layer에서 vector로 변환되고, 행렬곱으로 output을 반환한다 (batch_size, 1, max_len)\n",
        "\n",
        "output의 각 원소는 중심 단어 벡터와 문맥 혹은 noise 단어 벡터의 점곱이다."
      ],
      "metadata": {
        "id": "s9kRWz71jI2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
        "    v = embed_v(center)\n",
        "    u = embed_u(contexts_and_negatives)\n",
        "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
        "    return pred"
      ],
      "metadata": {
        "id": "S8lugWFxjIwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skip_gram(torch.ones((2, 1), dtype=torch.long),\n",
        "          torch.ones((2, 4), dtype=torch.long), embed, embed).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "668i3b5-i4k-",
        "outputId": "97c66235-7409-40c1-c865-7227d17bfc04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 1, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Training"
      ],
      "metadata": {
        "id": "qgP5qJBcjxuh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (1) Binary Cross-Entropy Loss"
      ],
      "metadata": {
        "id": "HdXVIlzXj4pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SigmoidBCELoss(nn.Module):\n",
        "    # Binary cross-entropy loss with masking\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, inputs, target, mask=None):\n",
        "        out = nn.functional.binary_cross_entropy_with_logits(\n",
        "            inputs, target, weight=mask, reduction=\"none\")\n",
        "        return out.mean(dim=1)\n",
        "\n",
        "loss = SigmoidBCELoss()"
      ],
      "metadata": {
        "id": "44rWHKUkjwom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = torch.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)\n",
        "label = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])\n",
        "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])\n",
        "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na5834Sqj8DB",
        "outputId": "c805524e-f78d-4152-b9e7-89f5dfc3c773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9352, 1.8462])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (2) Initializing Model Parameters\n",
        "\n",
        "중심 단어와 문맥 단어로 사용될 때 단어 사전의 모든 단어에 대한 Embedding Layer를 정의했다.\n",
        "\n",
        "emb_size는 100으로 설정하자."
      ],
      "metadata": {
        "id": "YVPZf3KckD0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 100\n",
        "net = nn.Sequential(nn.Embedding(num_embeddings=len(vocab),\n",
        "                                 embedding_dim=embed_size),\n",
        "                    nn.Embedding(num_embeddings=len(vocab),\n",
        "                                 embedding_dim=embed_size))"
      ],
      "metadata": {
        "id": "GLyrUb5qkU4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (3) Defining the Training Loop"
      ],
      "metadata": {
        "id": "seyWBJgMkY1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data_iter, lr, num_epochs, device='cuda:0'):\n",
        "    # Embedding의 Weight를 xavier로 초기화\n",
        "    def init_weights(module):\n",
        "        if type(module) == nn.Embedding:\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "    # 초기화 적용\n",
        "    net.apply(init_weights)\n",
        "\n",
        "    # 모델을 cpu나 cuda에 올리고\n",
        "    net = net.to(device)\n",
        "    \n",
        "    # Optimizer 설정\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    # Animator는 보여주면 좋으니 설정해두고\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[1, num_epochs])\n",
        "\n",
        "    # loss를 저장할 공간을 2개로 생성 [0.0, 0.0]\n",
        "    metric = d2l.Accumulator(2)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Timer로 시간 측정하면서, 총 배치 길이 가져오고\n",
        "        timer, num_batches = d2l.Timer(), len(data_iter)\n",
        "\n",
        "        # data_iter에서 batch 단위로 데이터 추출\n",
        "        for i, batch in enumerate(data_iter):\n",
        "\n",
        "            # optimizer 초기화\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # batch에서 각각 추출하면서 device에 올리고, 변수에 저장\n",
        "            center, context_negative, mask, label = [\n",
        "                data.to(device) for data in batch]\n",
        "\n",
        "            # skip_gram에 중심 단어 토큰, 문맥+noise 단어 토큰, Emb1, Emb2 넣고\n",
        "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
        "\n",
        "            # Loss는 Masking을 적용해서 계산해서\n",
        "            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n",
        "                     / mask.sum(axis=1) * mask.shape[1])\n",
        "            \n",
        "            # gradient 계산\n",
        "            l.sum().backward()\n",
        "\n",
        "            # 파라미터 갱신\n",
        "            optimizer.step()\n",
        "\n",
        "            # metric에 현재 batch에 대한 상태 업로드\n",
        "            metric.add(l.sum(), l.numel())\n",
        "\n",
        "            # Animator에 추가\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                             (metric[0] / metric[1],))\n",
        "    # 학습 완료시의 Loss와 Metric 반환\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
        "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
      ],
      "metadata": {
        "id": "_XWhlHtDkVnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs = 0.002, 30\n",
        "train(net, data_iter, lr, num_epochs, 'cuda:0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "y_8w-rXNlr2N",
        "outputId": "c129db1f-82f4-4670-f77a-45f3c13378a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 259.00625 180.65625\" width=\"259.00625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 259.00625 180.65625 \nL 259.00625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 143.1 \nL 245.44375 143.1 \nL 245.44375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 77.081681 143.1 \nL 77.081681 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m05bc4b423c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"77.081681\" xlink:href=\"#m05bc4b423c\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(73.900431 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 110.754095 143.1 \nL 110.754095 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"110.754095\" xlink:href=\"#m05bc4b423c\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(104.391595 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 144.426509 143.1 \nL 144.426509 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.426509\" xlink:href=\"#m05bc4b423c\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 15 -->\n      <g transform=\"translate(138.064009 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 178.098922 143.1 \nL 178.098922 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.098922\" xlink:href=\"#m05bc4b423c\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(171.736422 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 211.771336 143.1 \nL 211.771336 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.771336\" xlink:href=\"#m05bc4b423c\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 25 -->\n      <g transform=\"translate(205.408836 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 245.44375 143.1 \nL 245.44375 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"245.44375\" xlink:href=\"#m05bc4b423c\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(239.08125 157.698438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <defs>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     </defs>\n     <g transform=\"translate(132.565625 171.376563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 50.14375 116.280612 \nL 245.44375 116.280612 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m24c8ef512f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m24c8ef512f\" y=\"116.280612\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.45 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(20.878125 120.079831)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 50.14375 84.986452 \nL 245.44375 84.986452 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m24c8ef512f\" y=\"84.986452\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.50 -->\n      <g transform=\"translate(20.878125 88.785671)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_17\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 50.14375 53.692292 \nL 245.44375 53.692292 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m24c8ef512f\" y=\"53.692292\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.55 -->\n      <g transform=\"translate(20.878125 57.49151)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_19\">\n      <path clip-path=\"url(#pb75a8e6932)\" d=\"M 50.14375 22.398131 \nL 245.44375 22.398131 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m24c8ef512f\" y=\"22.398131\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(20.878125 26.19735)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     </defs>\n     <g transform=\"translate(14.798437 84.807812)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_21\">\n    <path clip-path=\"url(#pb75a8e6932)\" d=\"M 44.748111 13.377273 \nL 46.086954 61.235413 \nL 47.425797 80.423389 \nL 48.764641 90.695438 \nL 50.103484 97.220641 \nL 50.14375 97.365483 \nL 51.482593 102.176036 \nL 52.821437 105.918287 \nL 54.16028 109.017284 \nL 55.499123 111.673417 \nL 56.837967 114.009887 \nL 56.878233 114.067901 \nL 58.217076 116.566324 \nL 59.555919 118.815894 \nL 60.894763 120.843108 \nL 62.233606 122.692007 \nL 63.57245 124.399969 \nL 63.612716 124.44597 \nL 64.951559 126.563605 \nL 66.290402 128.483803 \nL 67.629246 130.225423 \nL 68.968089 131.808322 \nL 70.306932 133.277215 \nL 70.347198 133.317781 \nL 71.686042 135.208637 \nL 73.024885 136.922727 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 143.1 \nL 50.14375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 245.44375 143.1 \nL 245.44375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 143.1 \nL 245.44375 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 245.44375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb75a8e6932\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"50.14375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Applying Word Embeddings"
      ],
      "metadata": {
        "id": "OB2FaEIjl3nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W2V 학습 이후, 단어 벡터들 사이의 코사인 유사도로 의미론적으로 유사한 input 단어들을 사용할 수 있다."
      ],
      "metadata": {
        "id": "-DIZi8I_l6tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_tokens(query_token, k, embed):\n",
        "    # Embedding 된 단어 들\n",
        "    W = embed.weight.data\n",
        "\n",
        "    # 특정 단어에 대한 Vector\n",
        "    x = W[vocab[query_token]]\n",
        "\n",
        "    # Cosine 유사도 계산\n",
        "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *\n",
        "                                      torch.sum(x * x) + 1e-9)\n",
        "    \n",
        "    # Cosine 유사도 중 top K개의 단어들을 뽑아와서\n",
        "    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n",
        "\n",
        "    # 유사도와 그 단어 출력\n",
        "    for i in topk[1:]:  # Remove the input words\n",
        "        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')\n",
        "\n",
        "get_similar_tokens('Book', 3, net[0])"
      ],
      "metadata": {
        "id": "YZ_3CrnVluUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Summary"
      ],
      "metadata": {
        "id": "kjS55_GEmhp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Binary Cross Entropy Loss와 Embedding Layer를 활용해서 Negative Sampling을 활용한 Skip-Gram 모델을 학습했다.\n",
        "\n",
        "- 단어 벡터들의 코사인 유사도를 기반으로 주어진 단어에 대한 의미론적으로 유사한 단어를 찾아내는 것을 포함하여 Word Embedding의 적용 가능"
      ],
      "metadata": {
        "id": "TYoY-9L5mjZX"
      }
    }
  ]
}