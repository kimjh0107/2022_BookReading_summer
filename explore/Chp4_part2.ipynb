{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 이론은 ppt 참고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2. High-Dimensional Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose our label to be a linear function of our inputs, corrupted by Gaussian noise with zero mean and standard deviation 0.01. To make the effects of overfitting pronounced, we can increase the dimensionality of our problem to d=200 and work with a small training set containing only 20 examples.\n",
    "\n",
    "-> overfitting 되어 있는 다항식 만들어주기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"Generate y = Xw + b + noise\"\"\"\n",
    "    X = d2l.normal(0, 1, (num_examples, len(w)))\n",
    "    y = d2l.matmul(X, w) + b\n",
    "    y += d2l.normal(0, 0.01, y.shape) # = noise 에 해당 \n",
    "    return X, d2l.reshape(y, (-1, 1))\n",
    "\n",
    "\n",
    "n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5\n",
    "true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "\n",
    "train_data = d2l.synthetic_data(true_w, true_b, n_train)\n",
    "train_iter = d2l.load_array(train_data, batch_size) # dataloader \n",
    "test_data = d2l.synthetic_data(true_w, true_b, n_test)\n",
    "test_iter = d2l.load_array(test_data, batch_size, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model parameters \n",
    "# 이는 알고리즘은 학습 중인 데이터의 입력에서 출력까지 특정 매핑 기능에 좋은 weight 집단을 찾기 위해서 임의성을 사용해서 진행하기 때문 \n",
    "# 하는 이유는 학습을 위해 확률적 경사 하강법을 사용하는데 이 확률적 최적화 알고리즘은 탐색의 시작점을 선택하고 탐색을 진행할 때 임의성을 사용 \n",
    "# 그래서 네트워크의 가중치가 작은 임의값 (0-1에서와 같이 0에 가까운)으로 초기화되어야 됨. \n",
    "def init_params():\n",
    "    w = torch.normal(0, 1, size=(num_inputs, 1), requires_grad=True)\n",
    "    b = torch.zeros(1, requires_grad=True)\n",
    "    return [w, b]\n",
    "\n",
    "# define L2 norm penalty \n",
    "def l2_penalty(w):\n",
    "    return torch.sum(w.pow(2)) / 2\n",
    "\n",
    "# train loop \n",
    "def train(lambd):\n",
    "    w, b = init_params()\n",
    "    net = lambda X: d2l.linreg(X, w, b) # d2l.matmul(X, w) + b\n",
    "    loss = d2l.squared_loss # (y_hat - d2l.reshape(y, y_hat.shape)) ** 2 / 2\n",
    "    num_epochs = 100 \n",
    "    lr = 0.003\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            l = loss(net(X), y) + lambd * l2_penalty(w)\n",
    "            l.sum().backward()\n",
    "            d2l.sgd([w, b], lr, batch_size)\n",
    "\n",
    "    print('L2 norm of w:', torch.norm(w).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w: 13.211760520935059\n"
     ]
    }
   ],
   "source": [
    "train(lambd=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w: 0.3699600100517273\n"
     ]
    }
   ],
   "source": [
    "train(lambd=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.4. Concise Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w: 14.493910789489746\n"
     ]
    }
   ],
   "source": [
    "def train_concise(wd):\n",
    "    net = nn.Sequential(nn.Linear(num_inputs, 1))\n",
    "    for param in net.parameters():\n",
    "        param.data.normal_()\n",
    "    loss = nn.MSELoss(reduction='none')\n",
    "    num_epochs, lr = 100, 0.003\n",
    "    # The bias parameter has not decayed\n",
    "    trainer = torch.optim.SGD([\n",
    "        {\"params\":net[0].weight,'weight_decay': wd},\n",
    "        {\"params\":net[0].bias}], lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.mean().backward()\n",
    "            trainer.step()\n",
    "    print('L2 norm of w:', net[0].weight.norm().item())\n",
    "\n",
    "train_concise(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm of w: 0.5018966794013977\n"
     ]
    }
   ],
   "source": [
    "train_concise(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
